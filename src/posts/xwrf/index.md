---
title: 'Making WRF output data pythonic using xWRF'
date: '2022-09-20'
authors:
  - name: Jon Thielen
    github: jthielen
  - name: Lukas Pilz
    github: lpilz
---

![xWRF logo](https://github.com/xarray-contrib/xwrf/blob/main/docs/source/_static/xwrf_logo_bg_light.svg)

The [Weather Research and Forecasting model (`WRF`)](https://www.mmm.ucar.edu/weather-research-and-forecasting-model) is a mesoscale numerical weather prediction system, used for atmospheric research and operational forecasting.
Among other private meteorological organizations, it is currently used by [NCEP](https://weather.gov/ncep/).

`WRF` produces [NetCDF](https://www.unidata.ucar.edu/software/netcdf/) files for output, which have some custom quirks.
In order to post-process these files with python, the tool of choice would be the [`wrf-python`](https://wrf-python.readthedocs.io/en/latest/) package.
While `wrf-python` has a rich set of functionality, it also has some detrimental aspects which inspired a move away from it.
One such aspects is its lack of direct integration with distributed/parallel tools, such as [`dask`](https://www.dask.org/), which enable the ability to work these larger terabyte or possibly even petabyte scale datasets.

Here, we present to you [`xWRF`](https://github.com/xarray-contrib/xwrf/) (v0.0.2), a lightweight interface for working with the model output in [Xarray](https://docs.xarray.dev/en/stable/).
It is the successor of an earlier version of `xWRF` presented in the [NCAR-ESDS blog](https://ncar.github.io/esds/posts/2021/xarray-wrf-example/) in October 2021.
Since then, the package has matured considerably and now enables a seamless integration of the unique `WRF` data format into Xarray and the [Pangeo](https://pangeo.io/) software stack.
It achieves this by:

1. transforming `WRF` data into CF- and COMODO-compliant xarray datasets
2. converting `WRF` units into [`pint`](https://pint.readthedocs.io/en/stable/)-friendly ones
3. generating projection coordinates and providing a [`pyproj.CRS`](https://pyproj4.github.io/pyproj/dev/api/crs/crs.html) object

In this post, we will show how `xWRF` works together with other utilities in order to post-process a large `WRF` output dataset.

+++

## Installing `xWRF`

Before we start using `xWRF`, we need to install it. We can do this by following these steps!

1. Install this in your environment using either
   - [conda](https://anaconda.org/conda-forge/xwrf) (`conda install -c conda-forge xwrf`)
   - or [pip](https://pypi.org/project/xwrf/) (`pip install xwrf`)
2. Open up a notebook and use the imports shown below!

```{code-cell} ipython3
# for WRF data processing (`xwrf` Dataset and DataArray accessor)
import xwrf
# for unit conversion (`pint` DataArray accessor)
import pint_xarray
# for `dask`-accelerated computation
from distributed import LocalCluster, Client
# for numerics
import numpy as np
# for visualization
import holoviews as hv
import hvplot
import hvplot.xarray

hv.extension('bokeh') # set plotting backend
```

## Spin up a Cluster

In order to make use of `dask`'s distributed computational speedup, we create a cluster object.
If we had access to a HPC system, we could also use [`Dask-Jobqueue`](https://jobqueue.dask.org/en/latest/) in order to outsource the computation there.
Since we do not in this instance, we use a [`LocalCluster`](https://distributed.dask.org/en/stable/api.html#distributed.LocalCluster) instead.

```{code-cell} ipython3
n_workers = 4
cluster = LocalCluster(
        n_workers=n_workers,
        processes=True,
        threads_per_worker=1
    )
client = Client(cluster)
client.wait_for_workers(n_workers=n_workers)
client
```

## Example analysis

In this example, we will have a closer look at some `WRF` output data generated by downscaling the [`CMIP6` GCM](https://mpimet.mpg.de/en/science/projects/integrated-activities/translate-to-englisch-cmip6-das-gekoppelte-modellvergleichsprojekt) data to higher spatial resolutions.
The Coupled Model Intercomparison Project phase 6 (`CMIP6`) provides scientific input to the 6th assessment report of the IPCC ([IPCC AR6](https://www.ipcc.ch/report/ar6/wg1/)).
There, the `CMIP`-models are used to analyze the impact of different forcings on the climate system and predict future climate change given different scenarios.

Let's just imagine that one would want to investigate e.g. how climate change might affect the jet stream in the California region in different climate change scenarios.
In the AR6, these scenarios are called [SSP](https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change/).
One comparison one might want to make could be between the SSP5-8.5 and the SSP2-4.5 scenarios.

In order to be able to easily access this data, we are using an [`intake`](https://intake.readthedocs.io/en/latest/) catalog.
This catalog points to [`kerchunk`](https://fsspec.github.io/kerchunk/) metadata which in turn points to this [Opendata AWS bucket](https://registry.opendata.aws/wrf-cmip6/).
However, while this cell might look foreign - effectively it is nothing more than a `xarray.open_mfdataset(<files>, engine='netcdf4', chunks='auto', concat_dim='Time', combine='nested')` call.

```{code-cell} ipython3
import intake
cat = intake.open_catalog("https://raw.githubusercontent.com/xarray-contrib/xwrf-data/main/catalogs/catalog.yml")
ssp5_ds = cat["xwrf-sample-ssp585"].to_dask()
ssp5_ds
```

As one can see, this output is not very helpful.
Heaps of necessary information like `WRF` model grid coordinates or coordinate index assignments are missing.
But we can use `xWRF` to very efficiently fix this.

```{code-cell} ipython3
%time
ssp5_ds = ssp5_ds.xwrf.postprocess()
ssp5_ds
```

The `xWRF` post-processing is done in no time at all thanks to it leveraging the `dask` integration and it's delayed computation feature.
This is despite the uncompressed dataset being close to 60GB in size.

```{code-cell} ipython3
ssp5_ds.nbytes*1E-9
```

Next to some cleanup of the coordinates, the `xWRF` post-processing includes the calculation of some basic diagnostics not included in `WRF` output, namely `air_pressure`, `air_potential_temperature`, `geopotential` and `geopotential_height`.
Since version 0.0.2, it also computes `wind_east` and `wind_north` - earth relative wind vector components.

Using `dask`, the computations for determining these diagnostics are delayed.
They will be executed upon using `.compute()`, `.persist()` or `.values` on the variable itself or a variable depending on it.

```{code-cell} ipython3
ssp5_ds.air_pressure
```

With the SSP5-8.5 data loaded, we can load the SSP2-4.5 data too and post-process it in a single line!

```{code-cell} ipython3
ssp2_ds = cat["xwrf-sample-ssp245"].to_dask().xwrf.postprocess()
ssp2_ds
```

Now, if we want to calculate the wind speeds from grid-relative wind vector components, we have to destagger them first.
This is because `WRF` simulates on an [Arakawa-C grid](http://amps-backup.ucar.edu/information/configuration/wrf_grid_structure.html), which is numerically advantageous but means that some variables have differing shapes.
(We could also use the already-destaggered earth-relative wind component, but where would be the fun in that? ;) )
Finally, we can define the pressure levels we want to interpolate the wind speeds to and use `xgcm.Grid.transform` in order to perform the vertical interpolation.

```{code-cell} ipython3
from metpy.calc import wind_speed
import xgcm

plevs = np.array([350., 300., 250., 200., 150., 100.]) # in hPa
wind_speeds_js = []

for ds in [ssp2_ds, ssp5_ds]:
    for field in ['U', 'V']:
        ds[field] = ds[field].xwrf.destagger().variable
    ds = ds.metpy.quantify()
    _wind_speed = wind_speed(ds.U, ds.V).metpy.dequantify()
    grid = xgcm.Grid(ds, periodic=False)
    _wind_speed = grid.transform(_wind_speed, 'Z', plevs, target_data=ds.air_pressure.pint.to('hPa').metpy.dequantify(), method='log').persist()
    wind_speeds_js.append(_wind_speed)
```

Now, we can simply subtract the wind speed data of the two simulations from one another, let `dask` compute the outcome and plot the data using `hvplot.quadmesh`.

```{code-cell} ipython3
ws_difference = (wind_speeds_js[0] - wind_speeds_js[1]).compute()
max_value = np.max(np.fabs(ws_difference)).item()

plot = ws_difference.hvplot.quadmesh(
    x='XLONG',
    y='XLAT',
    groupby=['Time', 'air_pressure'],
    widget_location='bottom',
    title='Wind speed difference between SSPs 245 and 585',
    cmap='seismic',
    clim=(-max_value,max_value),
    clabel='wind speed [m/s]',
    coastline=True,
    geo=True,
    rasterize=True,
    project=True
)
plot
```

![blogpost.gif](https://user-images.githubusercontent.com/14276158/191564410-1a7a83f0-3795-445a-9f5d-388ba225fa13.gif)

Finally, we clean up our workspace.

```{code-cell} ipython3
ssp5_ds.close(); ssp2_ds.close();
cluster.close(); client.close()
```
